<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->

<html>
	<head>
		<title>CustomHumans</title>
		<meta charset="utf-8" />

        
		<meta name="viewport" content="width=device-width, initial-scale=1" />
        <link rel="stylesheet" href="assets/css/bulma.min.css">
        <link rel="stylesheet" href="assets/css/bulma-carousel.min.css">
        <link rel="stylesheet" href="assets/css/bulma-slider.min.css">
        <link rel="stylesheet" href="assets/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <link rel="stylesheet" href="assets/css/font-awesome.min.css">
        <link rel="stylesheet" href="assets/css/index.css">
        <link rel="stylesheet" href="assets/css/style.css" type="text/css" media="screen,projection" />
        <link rel="stylesheet" href="assets/css/main.css" />
        <link rel="shortcut icon" href="images/opt_icon.png" />

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="assets/js/bulma-carousel.min.js"></script>
        <script src="assets/js/bulma-slider.min.js"></script>
        <script src="assets/js/index.js"></script>
        <script src="assets/js/video_comparison.js"></script>

	</head>
	<body>

		<!-- Home -->
        <div class="wrapper first style1">
            <article class="container" id="home">
                <h2>Learning Locally Editable Virtual Humans</h2>
                <br>
                <div class="row center">
                    <div class="author col l3 m6 s12"><a href="https://azuxmioy.github.io/" target="_blank">Hsuan-I Ho</a></div>
                    <div class="author col l3 m6 s12"><a href="https://lxxue.github.io/" target="_blank">Lixin Xue</a></div>
                    <div class="author col l3 m6 s12"><a href="https://ait.ethz.ch/people/song/" target="_blank">Jie Song</a></div>
                    <div class="author col l3 m6 s12"><a href="https://ait.ethz.ch/people/hilliges/" target="_blank">Otmar Hilliges</a></div>
                </div>
                <br>
                <div class="col">
                    <div style="width: 20%; margin: 0em auto 0em auto">
                        <a  href="https://ait.ethz.ch/"><img class="responsive-img" src="images/ait_logo.png"></a>
                    </div>
                    <div class="institute">
                        <a href="https://inf.ethz.ch/" target="_blank">Department of Computer Science</a>,
                        <a href="https://ethz.ch/en.html" target="_blank">ETH Zürich</a>
                    </div>

                </div>
                <br>

                <div class="row">
                    <div class="col-sm-6 col-sm-offset-3 text-center">
                        <ul class="nav nav-pills nav-justified">
                            <li>
                                <a href="https://ait.ethz.ch/projects/2023/custom-humans/paper.pdf">
                                    <img src="images/icon/Adobe_PDF_icon.svg" width="60px">
                                    <h4><strong>Paper<br>(CVPR 2023)</strong></h4>
                                </a>
                            </li>
                            <li>
                                <a href="https://www.youtube.com/watch?v=aT8ql5hB3ZM">
                                <img src="images/icon/Yt_icon_2017_print.png" width="60px">
                                    <h4><strong>Video</strong></h4>
                                </a>
                            </li>
                            <li>
                                <a href="#download">
                                <image src="images/icon/database-icon.svg" width="50px">
                                    <h4><strong>Dataset</strong></h4>
                                </a>
                            </li>                            
                            <li>
                                <a href="https://custom-humans.github.io" target="_blank">
                                    <image src="images/icon/github.png" width="60px">
                                    <h4><strong>Code<br>(Coming Soon)</strong></h4>
                                </a>
                            </li>
                        </ul>
                    </div>
                </div>
            </article>
        </div>


		<!-- Abstract -->
        <div class="wrapper style1">
            <article id="abstract">
                <div class="container">
                    <h2>Abstract</h2>
                    <p style="text-align:justify">
                        In this paper, we propose a novel hybrid representation and end-to-end trainable network architecture to model fully editable and customizable neural avatars. At the core of our work lies a representation that combines the modeling power of neural fields with the ease of use and inherent 3D consistency of skinned meshes. To this end, we construct a trainable feature codebook to store local geometry and texture features on the vertices of a deformable body model, thus exploiting its consistent topology under articulation. This representation is then employed in a generative auto-decoder architecture that admits fitting to unseen scans and sampling of realistic avatars with varied appearances and geometries. Furthermore, our representation allows local editing by swapping local features between 3D assets. To verify our method for avatar creation and editing, we contribute a new high-quality dataset, dubbed CustomHumans, for training and evaluation. Our experiments quantitatively and qualitatively show that our method generates diverse detailed avatars and achieves better model fitting performance compared to state-of-the-art methods.
                    </p>
                </div>
            </article>
        </div>

		<!-- Paper -->
        <div class="wrapper style1">
            <article id="paper">
                <div class="container">
                
                    <h2>Video</h2>
                    <div class="row center" style="margin: 2em auto 2em auto">
                        <div class="col s10 offset-s1">
                            <div class="youtube-wrapper">
                                <iframe allowfullscreen="allowfullscreen" src="https://www.youtube.com/embed/aT8ql5hB3ZM"></iframe>
                            </div>
                        </div>
                    </div>

                    <h2>Method</h2>
                    <div class="row center" style="width: 100%; margin: 2em auto 2em auto">
                        <div class="col l6 m6 s6">
                            <img class="responsive-img" src="images/rep.gif"/>
                        </div>
                        <div class="col l6 m6 s6">
                            <img class="responsive-img" src="images/query.jpg"/>
                        </div>
                    </div>

                    <h3>Controllable Hybrid Human Representation</h3>
                    <p style="text-align:justify">
                        While neural avatars can be highly realistic, the question of how to edit such avatars remains open.
                        To enable 3D avatars with high-fidelity representational power and local editing capabilities, 
                        we propose a novel hybrid representation that combines the advantages of neural fields
                        (flexibility and modeling power) with LBS-articulated mesh models (ease of deformation and full explicit control).
                        We construct a trainable feature codebook which stores local texture and high-fidelity geometry respectively for each vertex.
                        Training and inference require queries of these features.
                        A query point <i>x<sub>g</sub></i> is projected onto the nearest triangle.
                        The global coordinates are then transformed into triangle coordinates <i>x<sub>l</sub></i>.
                        The triangle’s vertex indices are used to retrieve local texture and geometry features. 

                    </p>

                    <div class="row center" style="width: 100%; margin: 2em auto 2em auto">
                        <div class="col s10 offset-s1">
                            <img class="responsive-img" src="images/fitting.gif" style="width: 100%"/>
                        </div>                    
                    </div>
                    <h3>Avatar Customization by Feature Inversion</h3>
                    <p style="text-align:justify">
                        Our method allows for creating and personalizing avatars with diverse body shapes, appearances, and local
                        details. We leverage the above representation to train a multi-subject model which enables the transfer
                        of local features across subjects. We note that since the mesh topology of the LBS model is identical, this enables
                        us to learn a shared feature space from multiple posed scans. Given a trained model, our method can inverse any unseen 
                        3D scans into feature codebooks. This allows us to locally change either the clothing geometry or appearance of neural 
                        avatars given the corresponding feature indices. It is worth noting that resulting avatars enable detailed pose control 
                        via the SMPL-X parameters without affecting the fitted texture and geometry.
                    </p>
                </div>
            </article>
        </div>




        <!-- Download -->

        <div class="wrapper style1">
            <article id="download">
                <div class="container">
                    <h2>Download</h2>
                    <div class="table-wrapper" align="center">
                        <table>
                            <thead>
                                <tr>
                                    <th></th>
                                    <th>CustomHumans Dataset</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Download</strong></td>
                                    <td><h6><a href="https://forms.gle/oY4PKUyhH6Qqd5YA9" target="_blank">Application Link</a></h6></td>
                                </tr>
                                <tr>
                                    <td><strong>Number of Scans</strong></td>
                                    <td><h6>605</h6></td>
                                </tr>
                                <tr>
                                    <td><strong>Number of Subjects</strong></td>
                                    <td><h6>78</h6></td>
                                </tr>
                                <tr>
                                    <td><strong>Number of Garments</strong></td>
                                    <td><h6>120</h6></td>
                                </tr>
                                <tr>
                                    <td><strong>Registration</strong></td>
                                    <td><h6>SMPL-X</h6></td>
                                </tr>

                            </tbody>
                        </table>
                    </div>
                </div>

                <div class="container">
                    <div id="mesh-carousel" class="carousel results-carousel" style="height: 150%">
                        <div class="item">
                            <video poster="" autoplay controls muted loop height="150%">
                                <source src="images/dataset/001.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="item">
                            <video poster="" autoplay controls muted loop height="150%">
                                <source src="images/dataset/002.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="item">
                            <video poster="" autoplay controls muted loop height="150%">
                                <source src="images/dataset/003.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="item">
                            <video poster="" autoplay controls muted loop height="150%">
                                <source src="images/dataset/004.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="item">
                            <video poster="" autoplay controls muted loop height="150%">
                                <source src="images/dataset/005.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="item">
                            <video poster="" autoplay controls muted loop height="150%">
                                <source src="images/dataset/006.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="item">
                            <video poster="" autoplay controls muted loop height="150%">
                                <source src="images/dataset/007.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="item">
                            <video poster="" autoplay controls muted loop height="150%">
                                <source src="images/dataset/008.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="item">
                            <video poster="" autoplay controls muted loop height="150%">
                                <source src="images/dataset/009.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="item">
                            <video poster="" autoplay controls muted loop height="150%">
                                <source src="images/dataset/010.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                </div>


            </article>
        </div>

        <!-- Results -->


        <div class="wrapper style1">
            <article id="results">
                <div class="container">
                    <h2>Results</h2>
                    <h3>Avatar Customzation</h3>
                    <div class="row center" style="margin-top: 1em" >
                        <div class="col l4 m6 s12">
                            <video class="video" id="mesh1" loop autoplay muted src="images/001.mp4" onplay="resizeAndPlay(this)"></video>
                            <canvas height=0 class="videoMerge" id="mesh1Merge"></canvas>
                        </div>
                        <div class="col l4 m6 s12">
                            <video class="video" id="mesh2" loop autoplay muted src="images/009.mp4" onplay="resizeAndPlay(this)"></video>
                            <canvas height=0 class="videoMerge" id="mesh2Merge"></canvas>
                        </div>
                        <div class="col l4 m6 s12">
                            <video class="video" id="mesh3" loop autoplay muted src="images/048.mp4" onplay="resizeAndPlay(this)"></video>
                            <canvas height=0 class="videoMerge" id="mesh3Merge"></canvas>
                        </div>
                    </div>
                    <h3>Model Fitting Comparison</h3>
                    <div class="row center" style="width: 80%; margin: 1em auto 1em auto" >
                        <img src="images/fitting_compare.jpg">
                    </div>

                </div> 
            </article>
        </div>


        <!-- Reference -->
        <div class="wrapper style2">
            <article id="references" class="container">
                <header>
                    <h3>References</h3>
                </header>
                <ul class="default" style="text-align:left">
                    <li>
                        Chen et. al, "<a href="https://xuchen-ethz.github.io/gdna/" target="_blank">gDNA: Towards Generative Detailed Neural Avatars</a>." CVPR, 2022.
                    </li>
                    <li>
                        Palafox et. al, "<a href="https://pablopalafox.github.io/npms/" target="_blank">NPMs: Neural Parametric Models for 3D Deformable Shapes</a>." ICCV, 2021.
                    </li>
                    <li>
                        Palafox et. al, "<a href="http://www.iri.upc.edu/people/ecorona/smplicit/" target="_blank">SMPLicit: Topology-aware Generative Model for Clothed People</a>." CVPR, 2021.
                    </li>
                    <li>
                        Yu et. al, "<a href="https://github.com/ytrock/THuman2.0-Dataset" target="_blank">Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer RGBD Sensors</a>." CVPR, 2021.
                    </li>
                    <li>
                        Tiwari et. al, "<a href="https://virtualhumans.mpi-inf.mpg.de/sizer/" target="_blank">SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing</a>." ECCV, 2020.
                    </li>

                </ul>
                <div class="subtitle">
                    <h3>Bibtex</h3>
<pre><cite>@inproceedings{ho2023custom,
    title={Learning Locally Editable Virtual Humans},
    author={Hsuan-I Ho, Lixin Xue, Jie Song, and Otmar Hilliges},
    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2023}
  }</cite></pre>
                </div>
            </article>


            
        </div>

        <!-- Acknowledgement -->
        <div class="wrapper style3">
            <article>
                <div class="container">
                    <h2>Acknowledgement</h2>
                    <p  style="text-align:left">
                        We express our gratitude to Stefan Walter and Dean Bakker for infrastructure support, Juan Zarate for managing the capture stage, and Deniz Yildiz and Laura Wülfroth for data capture assistance. Thanks to Andrew Searle for supporting the capturing system, and anonymous thanks to all dataset participants.                    </p>
                </div>
            </article>
        </div>
      
        <!-- Copyright -->
        <div class="wrapper last style4">
            <article class="container">
                <footer>
                    <ul id="copyright">
                        <li>&copy; 2023 Hsuan-I Ho.</li><li>Thanks: <a href="http://html5up.net" target="_blank">HTML5 UP</a> and Po-Chen Wu provides this template</li>
                    </ul>
                </footer>
            </article>
        </div>

		<!-- Scripts -->
        <script src="assets/js/jquery.min.js"></script>
        <script src="assets/js/jquery.scrolly.min.js"></script>
        <script src="assets/js/skel.min.js"></script>
        <script src="assets/js/skel-viewport.min.js"></script>
        <script src="assets/js/util.js"></script>
        <script src="assets/js/main.js"></script>
        <script src="assets/js/materialize.js"></script>
    </body>
</html>